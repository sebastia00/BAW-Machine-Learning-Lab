{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<h1>Lab Center – Hands-on Lab</h1>\n",
    "\n",
    "<h2>Session <font color=red>7461</font></h2>\n",
    "<h2>Session Title  <font color=red>Add Intelligence to Business Automation with IBM Business Automation Insights</font></h2>\n",
    "\n",
    "<b>Christophe Jolif</b>, IBM Digital Business Automation, Architecture & Development, christophe.jolif@fr.ibm.com<br>\n",
    "<b>Sebastian Carbajales</b>, IBM Digital Business Workflow, Architecture & Development, sebastia@ca.ibm.com\n",
    "\n",
    "</div>\n",
    "\n",
    "You have completed <b>Section 1</b> in the lab by inspecting the as-is process.  You are now ready to work with the historical data generated by the process and create a machine-learning model that will provide a recommendation to approve or reject a loan request.\n",
    "\n",
    "<b>You will use this Python Jupyter notebook to accomplish this goal.</b>\n",
    "\n",
    "<h2>Jupyter Notebook Introduction</h2>\n",
    "\n",
    "A Jupyter notebook is a web-based environment for interactive computing. You can run small pieces of code that process your data, and you can immediately view the results of your computation. \n",
    "\n",
    "Notebooks include all of the building blocks you need to work with data:\n",
    "<ul>\n",
    "<li>Loading of the data\n",
    "<li>The code computations that process the data\n",
    "<li>Visualizations of the results\n",
    "<li>Text and rich media to enhance understanding\n",
    "</ul>\n",
    "\n",
    "Code computations can build upon each other to quickly unlock key insights from your data. Notebooks record how you worked with data, so you can understand exactly what was done, reproduce computations reliably, and share your findings with others.\n",
    "\n",
    "<h3>The cells in a Jupyter notebook</h3>\n",
    "\n",
    "A Jupyter notebook consists of a sequence of cells. The flow of a notebook is sequential. You enter code into an input cell, and when you run the cell, the notebook executes the code and prints the output of the computation to an output cell.\n",
    "\n",
    "You can change the code in an input cell and re-run the cell as often as you like. In this way, the notebook follows a read-evaluate-print loop paradigm.\n",
    "\n",
    "<h3>Useful Shortcuts</h3>\n",
    "\n",
    "Use the following shortcuts to execute the code in a cell:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <ul>\n",
    "        <li><b>Run cell: </b>CTRL + ENTER\n",
    "        <li><b>Run cell, select below: </b>SHIFT + ENTER\n",
    "    </ul>\n",
    "    <p>For a full list of shortcuts: <i><b>Help</b></i> > <i><b>Keyboard Shortcuts</b></i>\n",
    "</div>\n",
    "\n",
    "<h3>Do Want to Discover More?</h3>\n",
    "\n",
    "Take a took of the Jupyter notebook interface.  Launch it from <i><b>Help</b> > <b>User Interface Tour<b></i>.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h2> <font color=blue>Train and Deploy a Machine Learning Model to provide Loan Approval Recommendation</font>  </h2>\n",
    "\n",
    "You will now execute the code in this notebook to train and deploy a machine learning model.  You will integrate this model, in __Section 3__ of the lab, with the **Car Loan Approval** process to provide a recommendation to approve or reject a loan request.\n",
    "\n",
    "You will perform the following steps: \n",
    "\n",
    "1. [Load the time series data for the As-Is process](#step1)\n",
    "1. [Explore the format of the data and interpret it](#step2)\n",
    "1. [Create an Apache® Spark machine learning model](#step3)\n",
    "1. [Store the model in Watson ML](#step4)\n",
    "1. [Deploy a model](#step5)\n",
    "1. [Test the deployed model](#step6)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The code in this notebook is ready to execute, but you are encouraged to experiment with it by changing it and re-executing cells to see the effect of your change.<br>\n",
    "    \n",
    "Should you need to undo your changes you can revert to a previous checkpoint using the _**File** > **Revert to checkpoint**_ menu.\n",
    "</div>\n",
    "\n",
    "The following cell contains all the dependencies required to run this notebook.  The code can be uncommented and executed for a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install PySpark\n",
    "# !rm -rf $PIP_BUILD/pyspark\n",
    "# !pip install --upgrade pyspark==2.1.3\n",
    "\n",
    "## Install visualization packages\n",
    "# !pip install --upgrade matplotlib\n",
    "# !pip install --upgrade seaborna\n",
    "\n",
    "## Inspall Numpy\n",
    "# !pip install numpy\n",
    "\n",
    "## Insatll the Watson Machine learning API Package\n",
    "# !rm -rf $PIP_BUILD/watson-machine-learning-client\n",
    "# !pip install --upgrade watson-machine-learning-client==1.0.260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "## Step 1: Load the time series data for the As-Is process\n",
    "\n",
    "### The format of the IBM Business Automation Insights data\n",
    "\n",
    "Events emitted while a process executes are stored in **IBM Business Automation Insights**. Several event types are supported but in this scenario you only need the events that are recorded when a tracking point executes.  These are stored as **bpm-timeseries** for tracking data.  Every time a process executes a tracking point, a record is added to HDFS in the form of JSON data.\n",
    "\n",
    "In this scenario, the timeseries data is partitioned by the following elements:\n",
    "- The identifier and version number of the Workflow business process application\n",
    "- The tracking group identifier \n",
    "\n",
    "Thus, HDFS file names start with the following path:\n",
    "\n",
    "> _**[hdfs root]**/ibm-bai/bpmn-timeseries/**[processAppId]**/**[processAppVersionId]**/tracking/**[trackingGroupId]**_\n",
    "\n",
    "\n",
    "Remember, the tracking group name is **Loan_Approval**. To find the data, you query the various IDs from the Workflow system.\n",
    "\n",
    "> *Refer to https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_18.0.x/com.ibm.dba.bai/topics/ref_bai_data_paths.html for more details on HDFS data paths.* \n",
    "\n",
    "\n",
    "### Finding the application ID and version, and the tracking group ID\n",
    "\n",
    "You will use the **IBM Business Automation Workflow** REST API to retrieve the application and tracking group information.  You will then use these values to build the HDFS path described in the previous section.\n",
    "\n",
    "> *You can refer to https://www.ibm.com/support/knowledgecenter/en/SSYHZ8_18.0.x/com.ibm.dba.bai/topics/tsk_bai_retrieve_bpmn_id.html for more information on how to retrieve BPMN identifiers.*\n",
    "\n",
    "The Python code below sets up the REST API URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, requests, json\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "bpmusername='deadmin'\n",
    "bpmpassword='Think4me'\n",
    "bpmrestapiurl = 'https://ibmwin16.ibm.demo:9443/rest/bpm/wle/v1'\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=bpmusername, password=bpmpassword, verify=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now retrieve the _**process application ID and version number**_ by using the **processApps** REST API. The code below searches for the **'Lab 7461 - Car Loan Approval'** application and assumes that only one version or snapshot is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = bpmrestapiurl + '/processApps'\n",
    "response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "[processApp] = [x for x in json.loads(response.text).get('data').get('processAppsList') if x.get('name') == 'Lab 7461 - Car Loan Approval']\n",
    "\n",
    "processAppId = processApp.get('ID')\n",
    "\n",
    "# Note that the first 5 characters of the process app ID below are removed.  All BPM artifact IDs are prefixed\n",
    "# with the artifact type.  In this case, '2066.' indicates this is a process app ID.\n",
    "# The REST API returns the full process application id, including its prefix. \n",
    "\n",
    "print(\"Process application ID: \" + processAppId[5:])\n",
    "\n",
    "# Get the first snapshot - assume only one - this is the app version ID\n",
    "snapshot = processApp.get('installedSnapshots')[0]\n",
    "processAppVersionId = snapshot.get('ID')\n",
    "print(\"Process application version ID: \" + processAppVersionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you retrieve the _**tracking group ID**_ using the **'assets'** REST API.  You specify the process app ID, just computed, and asset type to filter the results to tracking groups defined within the **'Lab 7461 - Car Loan Approval'** application.  You then retrieve the **'Loan_Approval'** tracking group from the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = bpmrestapiurl + '/assets'\n",
    "\n",
    "response = requests.get(url, headers=headers, verify=False, params={'processAppId': processAppId, 'filter': 'type=TrackingGroup' })\n",
    "\n",
    "[trackingGroupId] = [x.get('poId') for x in json.loads(response.text).get('data').get('TrackingGroup') if x.get('name') == 'Loan_Approval']\n",
    "\n",
    "\n",
    "# Note that the first 3 characters of the tracking group ID below are removed. As in the process app case\n",
    "# this is the prefix to indicate this is a tracking group ID.\n",
    "\n",
    "print('Tracking group ID : ' + trackingGroupId[3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All required information to build the HDFS path has been obtained.  You now continue to query the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark SQL to read IBM Business Automation Insights data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IBM Business Automation Insights** stores data in HDFS. As described above, the events coming from the Workflow instance are stored in JSON files. \n",
    "\n",
    "The code below is already configured with the target HDFS URL for the lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hdfs_root = 'hdfs://hdfs1.ibm.edu/think2019'\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"dfs.client.use.datanode.hostname\", \"true\")\n",
    "\n",
    "# Get the timeseries Dataset by reading the JSON data from HDFS\n",
    "timeseries = spark.read.json(hdfs_root + \"/ibm-bai/bpmn-timeseries/\" + processAppId[5:] + '/' + processAppVersionId + '/tracking/' + trackingGroupId[3:] +  '/*/*')\n",
    "\n",
    "# If BAI were not available but you have the data in a loal file you can load it this way instead.\n",
    "# timeseries = spark.read.json(\"sample_loan_approval.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the various ids for the path are specified in the JSON path. This HDFS path could also use HDFS wildcards. Here, the * character replaces any directory or file name in the path.\n",
    "\n",
    "The data is loaded, let's take a quick look.  The code below will show a sample of the data, the schema and the number of records available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displays the top 20 rows of Dataset in a tabular form.\n",
    "timeseries.show()\n",
    "\n",
    "# Print the schema in a tree format\n",
    "timeseries.printSchema()\n",
    "\n",
    "# Finally, print the total number of events\n",
    "print ('The data containts ' + str(timeseries.count()) + ' events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we're interested in the data that was tracked by the process.  Looking at the schema, this data is contained within the **'trackedFields'** attribute of each even.\n",
    "\n",
    "The code below creates a temporary view on the data so that we can select just the tracked fields from the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creates a local temporary view called 'timeseries'\n",
    "timeseries.createOrReplaceTempView(\"timeseries\")\n",
    "\n",
    "# Select all tracked fields \n",
    "businessdata = spark.sql(\"SELECT trackedFields.* from timeseries\")\n",
    "\n",
    "# Displays the top 20 rows of Dataset in a tabular form.\n",
    "businessdata.show()\n",
    "\n",
    "# Print the schema in a tree format\n",
    "businessdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the column names by removing the type suffix from each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessdata = businessdata.withColumnRenamed(\"approved.string\", \"approved\")\n",
    "businessdata = businessdata.withColumnRenamed(\"creditScore.integer\", \"creditScore\")\n",
    "businessdata = businessdata.withColumnRenamed(\"requestedAmount.integer\", \"requestedAmount\")\n",
    "businessdata = businessdata.withColumnRenamed(\"approvedAmount.integer\", \"approvedAmount\")\n",
    "businessdata = businessdata.withColumnRenamed(\"vehicleMake.string\", \"vehicleMake\")\n",
    "businessdata = businessdata.withColumnRenamed(\"vehicleModel.string\", \"vehicleModel\")\n",
    "businessdata = businessdata.withColumnRenamed(\"vehicleType.string\", \"vehicleType\")\n",
    "businessdata = businessdata.withColumnRenamed(\"vehicleYear.integer\", \"vehicleYear\")\n",
    "businessdata.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step 2: Explore the format of the data and interpret it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you will learn a few techniques to understand the data that you are working with and determine what characteristics may be relevant to your prediction. In this exercise you want to be able to predict whether to approve or reject a loan request.  So you will look at the relationships between the **'approved'** field and the rest of them to determine which ones may be a good predictor.\n",
    "\n",
    "### Data manipulation libraries\n",
    "\n",
    "In the previous section you used Spark to load the data.  You will use the result to also train the Spark model.  However, in this section we convert the Spark dataframe to Pandas.  This is another table manipulation library but it plays well with visualization libraries. Spark, on the other hand, does not.  This lab uses the Seaborn visualization library.\n",
    "\n",
    "> _You can find more information on Pandas here http://pandas.pydata.org/pandas-docs/stable/._\n",
    "\n",
    "The Python code below creates a Pandas DataFrame from our Spark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_pd = businessdata.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a quick look at the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This prints just the columns in the data frame\n",
    "print(summary_pd.columns)\n",
    "\n",
    "# Use the head() function to preview the first n rows in the data frame.  If you don't pass\n",
    "# a value to the function, the default is 5.\n",
    "summary_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use visualization to analyze individual feature patterns\n",
    "\n",
    "First, we import the visualization packages \"Matplotlib\" and \"Seaborn\".  The last line, \"%matplotlib inline\", is required to plot in a Jupyter notebook.\n",
    "\n",
    "> *Refer to https://seaborn.pydata.org/ for more information on the Seaborn library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize individual fields it is important to understand the type of data you are dealing with to help you find the right visualization method.  the **'dtypes'** attribute returns the types in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using  Box Plots to see relationship between categorical and numerical variables\n",
    "\n",
    "In this lab we want to predict the **'approved'** field, so we want to understand the relationship between this field and the others to determine which will influence the value of **'approved'**.  Our target field is a categorical variable because it can only contain a value of true or false (1 or 0), as opposed to say, **'creditScore'** that can contain any value within a range.  **'creditScore'** is a numerical variable.\n",
    "\n",
    "A good way to visualize categorical variables is by using boxplots. Let's first examine the relationship between the **'approved'** and  **'vehicleYear'** fields.  The boxplot below shows this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"approved\", y=\"vehicleYear\", data=summary_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distributions of the vehicle's model year between the approved and rejected loan request categories have no significant overlap.  The distributions are distinct enough that **'vehicleYear'** is potentially a good predictor of **'approved'**.  \n",
    "\n",
    "On the other hand, if the overlap between distributions is significant then the that particular field would not be a good predictor of **'approved'**.  Change the value of __y__ in the code above to plot the distributions for _**'creditScore'**_ and _**'requestedAmount'**_ to determine whether those two are good predictors or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Categorical  Plots to see the breakdown in the distributions\n",
    "\n",
    "Categorical plots can be used to break down the distributions in a box plot into additional categorical variables.  The code below plots the approved/rejected distributions, for each vehicle make, against the vehicle model year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"vehicleMake\", y=\"vehicleYear\", hue=\"approved\", data=summary_pd, kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that the distribution for each make is different.  This would imply that vehicle make itself also has influence on the approval recommendation.\n",
    "\n",
    "Try changing the __x__ and **y** inputs to the plot to see the relationships between other fields.  Use categorical fields for x (vehicleMake, vehicleType) and use numerical fields for y (vehicleYear, requestedAmount, creditScore).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Characteristics\n",
    "\n",
    "We can take a look at the statistics of your data by using the __DataFrame.describe()__ function.  It will compute basic statistics for all variables.  If invoked with no arguments, it will analyze only continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display statistics for categorical variables by invoking it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_pd.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be interesting to look at the value counts within each category.  You can compute this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_pd['approved'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful in determining if we have enough entries for a given category.  For example, assume we had very few entries for SUVs, then vehicle type would not be a good predictor as it would skew the results.  As such, we may not be able to draw any conclusion about the vehicle type.\n",
    "\n",
    "Modify the code above to see the counts for the other categorical variables: **_vehicleType, vehicleMake, vehicleModel_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the data above against the **'approved'** variable by showing the counts of observations in each categorical bin using bars.  The code below plots the **'vehicleMake'** counts for approved and rejected loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"vehicleMake\", hue=\"approved\", data=summary_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing **x** to the other categorical variables: __*vehicleType and vehicleModel*__.\n",
    "\n",
    "### What have we observed?\n",
    "\n",
    "The techniques above gave us a picture of the data we are working with.  Using the box plots reveal that __vehicleYear__, __creditScore__ and __requestedAmount__, all numerical variables, can potentially be good predictors of approved.  The distribution of records between approved and rejected are different enough.\n",
    "\n",
    "Using categorical plots to look at further breakdown of the categories in these groups also reveal that the distributions are different enough that the categorical variables __vehicleMake__, __vehicleModel__ and __vehicleType__, are also potentially good predictors.\n",
    "\n",
    "Using the data characteristic analysis we can also confirm that there are enough samples in the data.  The value ranges in the numerical variables are wide, and the value counts on the categorical variables show that all possible categories are well represented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step 3: Create an Apache® Spark machine-learning model\n",
    "\n",
    "IBM Watson Machine learning supports a growing number of IBM or open-source machine-learning and deep-learning packages. This example uses Spark ML and, in particular, the Random Forest Classifier algorithm. In this section you will prepare the data, create an Apache® Spark machine-learning pipeline, and train the model.\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation of data\n",
    "\n",
    "In this section you will combine multiple complex algorithms into a single pipeline. These algorithms will be applied to the training data as well as the data supplied to the trained model to obtains the prediction, resulting in a simpler code when invoking the model.\n",
    "\n",
    "Our pipeline will include the following stages:\n",
    "1. Indexers\n",
    "1. Encoder\n",
    "1. Assembler\n",
    "1. Label converter\n",
    "1. Random Forest ML model\n",
    "\n",
    "> *More information on this and the Spark ML library can be found here: https://spark.apache.org/docs/2.1.0/ml-features.html*\n",
    "\n",
    "#### Indexers\n",
    "First set up the indexers whose job is to encode a string column of labels to a column of label indices. We'll use it to encode our categorical column into a numerical value.\n",
    "\n",
    "We use the **StringIndexer** which is a feature transformer.  We use it to accomplish two things:\n",
    "- Transforms the **'approved'** column, which is a column of type 'string' containing only 'true' or 'false' values, into a numeric column, **'label'**, with '0' and '1' values so that the classifier can understand it.\n",
    "- Transform the other categorical columns into a new set of index columns containing label indices.  The indices are in the the range of 0 to the number of labels for that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instanciate the indexer for the approved column\n",
    "approvalIndexer = StringIndexer(inputCol='approved', outputCol=\"label\").fit(businessdata)\n",
    "\n",
    "# Instanciate the rest of the indexers for the vehicle make, model and type columns.\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(businessdata) for column in list(set([\"vehicleMake\", \"vehicleModel\", \"vehicleType\"])) ]\n",
    "\n",
    "# Combine all indexers in a single list.\n",
    "indexers.append(approvalIndexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "We use the **OneHotEncoder** which is another feature transformer.   It maps a column of label indices to a column of binary vectors, with at most a single one-value. Here we use it to map the index columns we created in the previous stage for a vehicle's make, model and type.\n",
    "\n",
    "This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features such as _vehicleMake, vehicleModel and vehicleType._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_encoded\") for column in list(set([\"vehicleMake\", \"vehicleModel\", \"vehicleType\"])) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembler\n",
    "Next we set up a **VectorAssembler** which is a transformer that combines a given list of columns into a single vector column.  It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. \n",
    "\n",
    "We will train a random forest ML model so we will combine the features that we want to use in predicting our approval.\n",
    "\n",
    "We first define the set of features that we will use to predict the the **approved** field. We use all fields except for **approvedAmount** since the __Car Loan Approval__ process will not have that data when requesting a recommendation.  However, based on the analysis in the previous section, it makes sense to include all other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical columns we use the column name produced by the onehotencoder: <colName>_encoded\n",
    "features = [\"creditScore\", \"requestedAmount\",\"vehicleMake_encoded\",\"vehicleModel_encoded\",\"vehicleType_encoded\", \"vehicleYear\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label converter\n",
    "Finally, set up the **IndexToString** transformer which does the opposite of the StringIndexer.  It maps a column of label indices back to a column containing the original labels as strings.\n",
    "\n",
    "We use it to get the original label for our predicted value.  In other words, map '1' or '0' to 'true' or 'false'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=approvalIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating the model\n",
    "The model is built from the RandomForestClassifier algorithm.  We chose to use Random Forest because it is flexible and easy to use.  It produces great results even without hyperparameter tunning.  These are parameters that get set before the training begings and are used to optimize the model produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we split the data into training data and test data.  The prediction model is then trained and tested, and finally the accuracy of the model is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select the data\n",
    "businessdata = businessdata[[\"creditScore\", \"requestedAmount\",\"vehicleMake\",\"vehicleModel\",\"vehicleType\", \"vehicleYear\", \"approved\"]]\n",
    "\n",
    "# Split the data into a training and testing set (80/20)\n",
    "splitted_data = businessdata.randomSplit([0.8, 0.20], 24)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "# Instanciate the pipeline object, specifying all the satges we defined above\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf, labelConverter])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the result\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Step 4: Store the model in Watson ML\n",
    "Watson machine learning is used here to store the resulting model. After the model is stored, Watson machine learning makes it possible to create an HTTP scoring endpoint, which is then used as the recommendation service.\n",
    "\n",
    "The code below stores the created model and pipeline in IBM Watson Machine Learning. \n",
    "\n",
    "Lets start by importing the Watson Machine Learning API package.  \n",
    "\n",
    "> *Documentation on this API can be found here: https://wml-api-pyclient.mybluemix.net/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install watson_machine_learning_client \n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate a Watson machine learning client.  **Note** that if you are using your own instance of IBM Watson Machine Learning service you need to specify the authentication information for it in the cell below (wml_credentials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Authenticate to Watson Machine Learning service on IBM Cloud.\n",
    "\n",
    "wml_credentials={\n",
    "  \"apikey\": \"8aSfy7WbWV_7ioSe8kR4_tEc1ghS6Z07wkIflthGQmgT\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/96f925a37d236c8abd126579c5a53a7b:8f794cbe-79cd-4d90-acb2-f797eca3dbec::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-109020c5-c94e-457e-9246-9ee6f63f3a62\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/96f925a37d236c8abd126579c5a53a7b::serviceid:ServiceId-547f2b67-3197-4cae-8348-b98c9784b4e9\",\n",
    "  \"instance_id\": \"8f794cbe-79cd-4d90-acb2-f797eca3dbec\",\n",
    "  \"password\": \"6800c9a6-aee6-4bea-a59d-09511ae025c5\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "  \"username\": \"109020c5-c94e-457e-9246-9ee6f63f3a62\"\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the model and the training data. Call the **store_model** API to store trained model into Watson Machine Learning repository on Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "published_model_details = client.repository.store_model(model=model, meta_props={'name':'Recommendation Prediction Model'}, training_data=train_data, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step 5: Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is stored, we need to deploy it in a runtime environment, we start by retrieving the model uid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_uid = client.repository.get_model_uid(published_model_details)\n",
    "print(model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **deployments** client API to create a new deployment for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(artifact_uid=model_uid, name='Recommendation Prediction Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    Your model has been deployed and it is ready for use.  Take note of the <b><u>model_uid</u></b> and <b><u>deployment_uid</u></b> above.  You will need these values for <b>Section 3</b> of the lab.  These will be used by a Service Flow to make the REST call to the scoring API and obtain a recommendation for the <b>approved</b> field.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step 6: Testing the deployed model\n",
    "\n",
    "You can test the model using the **deployments** client API.\n",
    "\n",
    "The deployment details specifies the URL that will allow us to score against the published model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_url = client.deployments.get_scoring_url(deployment_details)\n",
    "\n",
    "print(recommendation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test using different input value. For categorical variables you must pass a value that is know to the model.  To quickly look at the different labels for each category you can use the following code to examine the Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "businessdata.groupBy(\"vehicleMake\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call the **deployments.score** API to get a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Declare summy input for the prediction\n",
    "recommendation_data = {\"fields\": [\"creditScore\", \"requestedAmount\", \"vehicleMake\", \"vehicleModel\", \"vehicleYear\", \"vehicleType\"],\n",
    "                       \"values\": [[350, 14654, \"GM\", \"Malibu\", 2011, \"Car\"]]}\n",
    "\n",
    "# Call the scoring API to predict the approval\n",
    "scoring_response = client.deployments.score(recommendation_url, recommendation_data)\n",
    "i = scoring_response['fields'].index('predictedLabel')\n",
    "j = scoring_response['fields'].index('probability')\n",
    "print(\"Recommend to approve = %s\" %scoring_response['values'][0][i])\n",
    "print(\"Confidence = %g\" %scoring_response['values'][0][j][1])\n",
    "\n",
    "# Uncomment to dump the full response\n",
    "# print(json.dumps(scoring_response, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Conclusion\n",
    "\n",
    "Using this notebook you have trained and deployed a machine learning model that can provide a recommendation to approve or reject a car loan request.  With the two ID's created in **step 5**, you can now continue to __Section 3__ in the lab instructions to complete the process application changes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
